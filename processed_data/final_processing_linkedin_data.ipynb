{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kagglehub\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = pd.read_csv('raw_linkedin_data/companies/companies.csv')\n",
    "employee_counts = pd.read_csv('raw_linkedin_data/companies/employee_counts.csv')\n",
    "industries = pd.read_csv('raw_linkedin_data/companies/company_industries.csv')\n",
    "specialities = pd.read_csv('raw_linkedin_data/companies/company_specialities.csv')\n",
    "industries_ids = pd.read_csv('raw_linkedin_data/mappings/industries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#did a simple check to see if there are duplicates in the data regarding industries\n",
    "#this is important because if there are duplicates, we need to process them\n",
    "#in order to avoid any errors in the data\n",
    "\n",
    "companies_has_duplicates = companies['company_id'].duplicated().any()\n",
    "print(\"Duplicates present in companies.csv:\", companies_has_duplicates)\n",
    "\n",
    "employee_counts_has_duplicates = employee_counts['company_id'].duplicated().any()\n",
    "print(\"Duplicates present in employee_counts.csv:\", employee_counts_has_duplicates)\n",
    "\n",
    "industry_has_duplicates = industries['company_id'].duplicated().any()\n",
    "print(\"Duplicates present in company_industries.csv:\", industry_has_duplicates)\n",
    "\n",
    "specialities_has_duplicates = specialities['company_id'].duplicated().any()\n",
    "print(\"Duplicates present in company_specialities.csv:\", specialities_has_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_counts['time_recorded'] = pd.to_datetime(employee_counts['time_recorded'])\n",
    "sorted_employee_counts = employee_counts.sort_values(['company_id', 'time_recorded'], ascending=[True, False])\n",
    "unique_employee_counts = sorted_employee_counts.drop_duplicates(subset='company_id', keep='first')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to the earlier step to see if we are getting the most recent employee counts\n",
    "max_dates = employee_counts.groupby('company_id')['time_recorded'].max().reset_index().rename(columns={'time_recorded': 'max_date'})\n",
    "merged = unique_employee_counts.merge(max_dates, on='company_id')\n",
    "merged['is_most_recent'] = merged['time_recorded'] == merged['max_date']\n",
    "all_recent = merged['is_most_recent'].all()\n",
    "print(\"All records are the most recent:\", all_recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industries_with_ids = industries.merge(\n",
    "    industries_ids,\n",
    "    left_on='industry',\n",
    "    right_on='industry_name',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#drop the original industry columns\n",
    "industries_with_ids = industries_with_ids.drop(columns=['industry', 'industry_name'])\n",
    "\n",
    "#convert  non-NaN values in 'industry_id' to int, and keep NaN as is\n",
    "industries_with_ids['industry_id'] = industries_with_ids['industry_id'].apply(\n",
    "    lambda x: int(x) if pd.notnull(x) else x\n",
    ")\n",
    "\n",
    "print(industries_with_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialities['speciality'] = specialities['speciality'].astype(str).str.strip()\n",
    "\n",
    "aggregated_industries_list = industries_with_ids.groupby('company_id')['industry_id'].apply(\n",
    "lambda x: sorted([i for i in list(x.unique()) if pd.notnull(i)])  # Filter out NaN values\n",
    ").reset_index()\n",
    "\n",
    "aggregated_specialities_list = specialities.groupby('company_id')['speciality'].apply(\n",
    "    lambda x: sorted(list(x.unique()))\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_company_data = companies.merge(unique_employee_counts, on='company_id', how='left') \\\n",
    "                        .merge(aggregated_industries_list, on='company_id', how='left') \\\n",
    "                        .merge(aggregated_specialities_list, on='company_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_company_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the postings dataframe\n",
    "\n",
    "postings = pd.read_csv('raw_linkedin_data/postings.csv')\n",
    "jobs_industries = pd.read_csv('raw_linkedin_data/jobs/job_industries.csv')\n",
    "job_salaries = pd.read_csv('raw_linkedin_data/jobs/salaries.csv')\n",
    "job_skills = pd.read_csv('raw_linkedin_data/jobs/job_skills.csv')\n",
    "\n",
    "benefits = pd.read_csv('raw_linkedin_data/jobs/benefits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_salary_dataset(postings):\n",
    "    salary_df = postings.copy()\n",
    "    \n",
    "    has_salary = (\n",
    "        salary_df['max_salary'].notna() | \n",
    "        salary_df['med_salary'].notna() | \n",
    "        salary_df['min_salary'].notna() |\n",
    "        salary_df['normalized_salary'].notna()\n",
    "    )\n",
    "    \n",
    "    has_period = salary_df['pay_period'].notna()\n",
    "    \n",
    "    salary_df = salary_df[has_salary & has_period]\n",
    "    \n",
    "    return salary_df\n",
    "\n",
    "postings_with_salary = create_salary_dataset(postings)\n",
    "print(f\"Original postings: {len(postings)}\")\n",
    "print(f\"Postings with salary: {len(postings_with_salary)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usd = (postings_with_salary['currency'] == 'USD').all()\n",
    "print(f\"All salaries in USD: {all_usd}\")\n",
    "\n",
    "\n",
    "currency_counts = postings_with_salary['currency'].value_counts()\n",
    "print(currency_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_usd_jobs = postings_with_salary[postings_with_salary['currency'] != 'USD']\n",
    "print(\"\\nID\\tLocation\\tCurrency\")\n",
    "print(\"_\"*50)\n",
    "for _, job in non_usd_jobs.iterrows():\n",
    "    print(f\"{job['job_id']}\\t{job['location']}\\t{job['currency']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since all the jobs that are not in USD are located in United States, we should convert the salaries to USD using these conversion rates.\n",
    "#1.00 US Dollar = 0.92367131 EUR\n",
    "#1.00 US Dollar = 1.4352374 CAD\n",
    "#1.00 US Dollar = 2.00 BBD\n",
    "#1.00 US Dollar = 1.594355 AUD\n",
    "#1.00 US Dollar = 0.77381294 GBP\n",
    "\n",
    "\n",
    "#converstion rates to USD\n",
    "currency_to_usd = {\n",
    "    'EUR': 1 / 0.92367131,\n",
    "    'CAD': 1 / 1.4352374,\n",
    "    'BBD': 1 / 2.00,\n",
    "    'AUD': 1 / 1.594355,\n",
    "    'GBP': 1 / 0.77381294,\n",
    "    'USD': 1.0  # No conversion needed\n",
    "}\n",
    "\n",
    "#convert all non-USD salaries to USD\n",
    "def convert_to_usd(row):\n",
    "    if row['currency'] != 'USD':\n",
    "        conversion_rate = currency_to_usd.get(row['currency'], 1.0)\n",
    "        \n",
    "        #convert all salary fields to USD\n",
    "        for field in ['max_salary', 'med_salary', 'min_salary', 'normalized_salary']:\n",
    "            if field in row and pd.notna(row[field]):\n",
    "                row[field] = row[field] * conversion_rate\n",
    "        \n",
    "        #update the currency to USD\n",
    "        row['currency'] = 'USD'\n",
    "    \n",
    "    return row\n",
    "\n",
    "#do this for all the rows\n",
    "postings_with_salary = postings_with_salary.apply(convert_to_usd, axis=1)\n",
    "\n",
    "#verify\n",
    "all_usd = (postings_with_salary['currency'] == 'USD').all()\n",
    "print(f\"All salaries converted to USD: {all_usd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_work_types = postings_with_salary['formatted_work_type'].unique()\n",
    "print(unique_work_types)\n",
    "\n",
    "unique_pay_periods = postings_with_salary['pay_period'].unique()\n",
    "print(unique_pay_periods)\n",
    "\n",
    "hourly_by_work_type = postings_with_salary[postings_with_salary['pay_period'] == 'HOURLY']['formatted_work_type'].value_counts()\n",
    "\n",
    "print(\"\\n\\nwork type:\")\n",
    "print(hourly_by_work_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#after carefully examining the data, we can see that the normalized salary is sometimes taking the med_salary when available, sometimes average of max and min, and sometimes the max_salary when the med_salary is not available. \n",
    "#this column might not be of use to use since it is not consistent. \n",
    "\n",
    "postings_with_salary = postings_with_salary.drop(columns=['normalized_salary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_normalized_annual_salaries(df):\n",
    "    # Create new columns\n",
    "    df['norm_min_annual'] = df['min_salary'].copy()\n",
    "    df['norm_med_annual'] = df['med_salary'].copy()\n",
    "    df['norm_max_annual'] = df['max_salary'].copy()\n",
    "    \n",
    "    #we need to convert all the salaries to annual salaries\n",
    "    pay_period_multipliers = {\n",
    "        'HOURLY': lambda row: 20 * 52 if row['work_type'] == 'PART_TIME' else 40 * 52,\n",
    "        'WEEKLY': 52,\n",
    "        'BIWEEKLY': 26,\n",
    "        'MONTHLY': 12,\n",
    "        'YEARLY': 1,\n",
    "        'ANNUAL': 1\n",
    "    }\n",
    "    \n",
    "    #apply converstions to annual salaries\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['pay_period'] in pay_period_multipliers:\n",
    "            work_pay_multiplier = pay_period_multipliers[row['pay_period']]\n",
    "            if callable(work_pay_multiplier):\n",
    "                work_pay_multiplier = work_pay_multiplier(row)\n",
    "                \n",
    "            for col in ['norm_min_annual', 'norm_med_annual', 'norm_max_annual']:\n",
    "                if pd.notna(row[col]):\n",
    "                    df.at[idx, col] = row[col] * work_pay_multiplier\n",
    "    \n",
    "    #any missing med salary, we can just do an average of the min and max salaries\n",
    "    mask = pd.isna(df['norm_med_annual']) & pd.notna(df['norm_min_annual']) & pd.notna(df['norm_max_annual'])\n",
    "    df.loc[mask, 'norm_med_annual'] = (df.loc[mask, 'norm_min_annual'] + df.loc[mask, 'norm_max_annual']) / 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "postings_with_salary = create_normalized_annual_salaries(postings_with_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "original_locations = []\n",
    "for loc in postings_with_salary['location'].dropna().unique():\n",
    "   if not re.search(r', [A-Z]{2}$', str(loc)):\n",
    "       original_locations.append(loc)\n",
    "\n",
    "\n",
    "original_locations.sort()\n",
    "print(len(original_locations))\n",
    "\n",
    "#looking at all the locations that are not in the standard format\n",
    "print(f\"Locations without standard format ({len(original_locations)} found):\")\n",
    "for loc in original_locations:\n",
    "   print(f\"- {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_locations(df):\n",
    "    #created a dictionary mappings for state names to their abbreviations\n",
    "    state_to_abbrev = {\n",
    "        'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', \n",
    "        'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n",
    "        'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID', \n",
    "        'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
    "        'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "        'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "        'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n",
    "        'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n",
    "        'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
    "        'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "        'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n",
    "        'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n",
    "        'Wisconsin': 'WI', 'Wyoming': 'WY', 'District of Columbia': 'DC'\n",
    "    }\n",
    "    \n",
    "    #dictionary to metro areas to city and state\n",
    "    metro_to_city_state = {\n",
    "    # A\n",
    "    \"Albany, New York Metropolitan Area\": \"Albany, NY\",\n",
    "    \"Albuquerque-Santa Fe Metropolitan Area\": \"Albuquerque, NM\",\n",
    "    \"Appleton-Oshkosh-Neenah Area\": \"Appleton, WI\",\n",
    "    \"Atlanta Metropolitan Area\": \"Atlanta, GA\",\n",
    "    \"Austin, Texas Metropolitan Area\": \"Austin, TX\",\n",
    "    \n",
    "    # B\n",
    "    \"Baton Rouge Metropolitan Area\": \"Baton Rouge, LA\",\n",
    "    \"Beaumont-Port Arthur Area\": \"Beaumont, TX\",\n",
    "    \"Bellingham Metropolitan Area\": \"Bellingham, WA\",\n",
    "    \"Blacksburg-Christiansburg-Radford Area\": \"Blacksburg, VA\",\n",
    "    \"Boise Metropolitan Area\": \"Boise, ID\",\n",
    "    \"Buffalo-Niagara Falls Area\": \"Buffalo, NY\",\n",
    "    \n",
    "    # C\n",
    "    \"Cape Coral Metropolitan Area\": \"Cape Coral, FL\",\n",
    "    \"Charleston, South Carolina Metropolitan Area\": \"Charleston, SC\",\n",
    "    \"Charlotte Metro\": \"Charlotte, NC\",\n",
    "    \"Cincinnati Metropolitan Area\": \"Cincinnati, OH\",\n",
    "    \"College Station-Bryan Area\": \"College Station, TX\",\n",
    "    \"Columbia, South Carolina Metropolitan Area\": \"Columbia, SC\",\n",
    "    \"Columbus, Ohio Metropolitan Area\": \"Columbus, OH\",\n",
    "    \"Crestview-Fort Walton Beach-Destin Area\": \"Fort Walton Beach, FL\",\n",
    "    \n",
    "    # D\n",
    "    \"Dallas-Fort Worth Metroplex\": \"Dallas, TX\",\n",
    "    \"Denver Metropolitan Area\": \"Denver, CO\",\n",
    "    \"Des Moines Metropolitan Area\": \"Des Moines, IA\",\n",
    "    \"Detroit Metropolitan Area\": \"Detroit, MI\",\n",
    "    \n",
    "    # E\n",
    "    \"Eau Claire-Menomonie Area\": \"Eau Claire, WI\",\n",
    "    \"Erie-Meadville Area\": \"Erie, PA\",\n",
    "    \n",
    "    # F\n",
    "    \"Fayetteville, North Carolina Metropolitan Area\": \"Fayetteville, NC\",\n",
    "    \n",
    "    # G\n",
    "    \"Grand Rapids Metropolitan Area\": \"Grand Rapids, MI\",\n",
    "    \"Greater Albany, Georgia Area\": \"Albany, GA\",\n",
    "    \"Greater Asheville\": \"Asheville, NC\",\n",
    "    \"Greater Augusta Area\": \"Augusta, GA\",\n",
    "    \"Greater Bend Area\": \"Bend, OR\",\n",
    "    \"Greater Birmingham, Alabama Area\": \"Birmingham, AL\",\n",
    "    \"Greater Bismarck Area\": \"Bismarck, ND\",\n",
    "    \"Greater Bloomington Area\": \"Bloomington, IN\",\n",
    "    \"Greater Boston\": \"Boston, MA\",\n",
    "    \"Greater Burlington Area\": \"Burlington, VT\",\n",
    "    \"Greater Chattanooga\": \"Chattanooga, TN\",\n",
    "    \"Greater Chicago Area\": \"Chicago, IL\",\n",
    "    \"Greater Chico Area\": \"Chico, CA\",\n",
    "    \"Greater Cleveland\": \"Cleveland, OH\",\n",
    "    \"Greater Colorado Springs Area\": \"Colorado Springs, CO\",\n",
    "    \"Greater Corpus Christi Area\": \"Corpus Christi, TX\",\n",
    "    \"Greater Dothan\": \"Dothan, AL\",\n",
    "    \"Greater Enid Area\": \"Enid, OK\",\n",
    "    \"Greater Eugene-Springfield Area\": \"Eugene, OR\",\n",
    "    \"Greater Fayetteville, AR Area\": \"Fayetteville, AR\",\n",
    "    \"Greater Flagstaff Area\": \"Flagstaff, AZ\",\n",
    "    \"Greater Fort Collins Area\": \"Fort Collins, CO\",\n",
    "    \"Greater Fort Wayne\": \"Fort Wayne, IN\",\n",
    "    \"Greater Goldsboro Area\": \"Goldsboro, NC\",\n",
    "    \"Greater Grand Junction Area\": \"Grand Junction, CO\",\n",
    "    \"Greater Hartford\": \"Hartford, CT\",\n",
    "    \"Greater Houston\": \"Houston, TX\",\n",
    "    \"Greater Indianapolis\": \"Indianapolis, IN\",\n",
    "    \"Greater Jackson, MI Area\": \"Jackson, MI\",\n",
    "    \"Greater Lansing\": \"Lansing, MI\",\n",
    "    \"Greater Lexington Area\": \"Lexington, KY\",\n",
    "    \"Greater Macon\": \"Macon, GA\",\n",
    "    \"Greater Madison Area\": \"Madison, WI\",\n",
    "    \"Greater McAllen Area\": \"McAllen, TX\",\n",
    "    \"Greater Milwaukee\": \"Milwaukee, WI\",\n",
    "    \"Greater Minneapolis-St. Paul Area\": \"Minneapolis, MN\",\n",
    "    \"Greater Morgantown Area\": \"Morgantown, WV\",\n",
    "    \"Greater New Orleans Region\": \"New Orleans, LA\",\n",
    "    \"Greater Orlando\": \"Orlando, FL\",\n",
    "    \"Greater Philadelphia\": \"Philadelphia, PA\",\n",
    "    \"Greater Phoenix Area\": \"Phoenix, AZ\",\n",
    "    \"Greater Pittsburgh Region\": \"Pittsburgh, PA\",\n",
    "    \"Greater Reno Area\": \"Reno, NV\",\n",
    "    \"Greater Richmond Region\": \"Richmond, VA\",\n",
    "    \"Greater Sacramento\": \"Sacramento, CA\",\n",
    "    \"Greater San Luis Obispo Area\": \"San Luis Obispo, CA\",\n",
    "    \"Greater Savannah Area\": \"Savannah, GA\",\n",
    "    \"Greater Scranton Area\": \"Scranton, PA\",\n",
    "    \"Greater Seattle Area\": \"Seattle, WA\",\n",
    "    \"Greater Sioux Falls Area\": \"Sioux Falls, SD\",\n",
    "    \"Greater St. Louis\": \"St. Louis, MO\",\n",
    "    \"Greater Syracuse-Auburn Area\": \"Syracuse, NY\",\n",
    "    \"Greater Tampa Bay Area\": \"Tampa, FL\",\n",
    "    \"Greater Tucson Area\": \"Tucson, AZ\",\n",
    "    \"Greater Wilmington Area\": \"Wilmington, DE\",\n",
    "    \"Green Bay, Wisconsin Metropolitan Area\": \"Green Bay, WI\",\n",
    "    \"Greensboro--Winston-Salem--High Point Area\": \"Greensboro, NC\",\n",
    "    \"Greenville-Spartanburg-Anderson, South Carolina Area\": \"Greenville, SC\",\n",
    "    \n",
    "    # H\n",
    "    \"Hampton Roads, Virginia Metropolitan Area\": \"Norfolk, VA\",\n",
    "    \"Hilton Head Island, South Carolina Area\": \"Hilton Head Island, SC\",\n",
    "    \"Honolulu Metropolitan Area\": \"Honolulu, HI\",\n",
    "    \n",
    "    # J\n",
    "    \"Johnson City-Kingsport-Bristol Area\": \"Johnson City, TN\",\n",
    "    \n",
    "    # K\n",
    "    \"Kansas City Metropolitan Area\": \"Kansas City, MO\",\n",
    "    \"Knoxville Metropolitan Area\": \"Knoxville, TN\",\n",
    "    \n",
    "    # L\n",
    "    \"La Crosse-Onalaska Area\": \"La Crosse, WI\",\n",
    "    \"Lafayette, Indiana Metropolitan Area\": \"Lafayette, IN\",\n",
    "    \"Lafayette, Louisiana Metropolitan Area\": \"Lafayette, LA\",\n",
    "    \"Las Vegas Metropolitan Area\": \"Las Vegas, NV\",\n",
    "    \"Lawton Area\": \"Lawton, OK\",\n",
    "    \"Lincoln, Nebraska Metropolitan Area\": \"Lincoln, NE\",\n",
    "    \"Little Rock Metropolitan Area\": \"Little Rock, AR\",\n",
    "    \"Los Angeles Metropolitan Area\": \"Los Angeles, CA\",\n",
    "    \"Louisville Metropolitan Area\": \"Louisville, KY\",\n",
    "    \"Lubbock-Levelland Area\": \"Lubbock, TX\",\n",
    "    \n",
    "    # M\n",
    "    \"Maui\": \"Lahaina, HI\",\n",
    "    \"Memphis Metropolitan Area\": \"Memphis, TN\",\n",
    "    \"Metro Jacksonville\": \"Jacksonville, FL\",\n",
    "    \"Metropolitan Fresno\": \"Fresno, CA\",\n",
    "    \"Miami-Fort Lauderdale Area\": \"Miami, FL\",\n",
    "    \"Mobile Metropolitan Area\": \"Mobile, AL\",\n",
    "    \"Modesto-Merced Area\": \"Modesto, CA\",\n",
    "    \n",
    "    # N\n",
    "    \"Nashville Metropolitan Area\": \"Nashville, TN\",\n",
    "    \"New Bern-Morehead City Area\": \"New Bern, NC\",\n",
    "    \"New York City Metropolitan Area\": \"New York, NY\",\n",
    "    \n",
    "    # O\n",
    "    \"Oklahoma City Metropolitan Area\": \"Oklahoma City, OK\",\n",
    "    \"Omaha Metropolitan Area\": \"Omaha, NE\",\n",
    "    \n",
    "    # P\n",
    "    \"Pensacola Metropolitan Area\": \"Pensacola, FL\",\n",
    "    \"Peoria Metropolitan Area\": \"Peoria, IL\",\n",
    "    \"Portland, Maine Metropolitan Area\": \"Portland, ME\",\n",
    "    \"Portland, Oregon Metropolitan Area\": \"Portland, OR\",\n",
    "    \"Pueblo-Ca√±on City Area\": \"Pueblo, CO\",\n",
    "    \n",
    "    # R\n",
    "    \"Raleigh-Durham-Chapel Hill Area\": \"Raleigh, NC\",\n",
    "    \"Rochester, New York Metropolitan Area\": \"Rochester, NY\",\n",
    "    \"Rocky Mount-Wilson Area\": \"Rocky Mount, NC\",\n",
    "    \n",
    "    # S\n",
    "    \"Salt Lake City Metropolitan Area\": \"Salt Lake City, UT\",\n",
    "    \"San Antonio, Texas Metropolitan Area\": \"San Antonio, TX\",\n",
    "    \"San Diego Metropolitan Area\": \"San Diego, CA\",\n",
    "    \"San Francisco Bay Area\": \"San Francisco, CA\",\n",
    "    \"South Bend-Mishawaka Region\": \"South Bend, IN\",\n",
    "    \"Springfield, Illinois Metropolitan Area\": \"Springfield, IL\",\n",
    "    \"Springfield, Massachusetts Metropolitan Area\": \"Springfield, MA\",\n",
    "    \n",
    "    # T\n",
    "    \"Tallahassee Metropolitan Area\": \"Tallahassee, FL\",\n",
    "    \"Toledo, Ohio Metropolitan Area\": \"Toledo, OH\",\n",
    "    \"Topeka Metropolitan Area\": \"Topeka, KS\",\n",
    "    \"Tulsa Metropolitan Area\": \"Tulsa, OK\",\n",
    "    \n",
    "    # U\n",
    "    \"Utica-Rome Area\": \"Utica, NY\",\n",
    "    \n",
    "    # W\n",
    "    \"Washington DC-Baltimore Area\": \"Washington, DC\",\n",
    "    \"Waterloo-Cedar Falls Area\": \"Waterloo, IA\",\n",
    "    \"Wichita, Kansas Metropolitan Area\": \"Wichita, KS\",\n",
    "    \"Walla Walla Area\" : \"Walla Walla, WA\",\n",
    "    \n",
    "    # Y\n",
    "    \"Youngstown-Warren area\": \"Youngstown, OH\"\n",
    "}\n",
    "    \n",
    "    def standardize_location(location):\n",
    "        if pd.isna(location):\n",
    "            return location\n",
    "            \n",
    "        location = str(location).strip()\n",
    "        \n",
    "        if location in metro_to_city_state:\n",
    "            return metro_to_city_state[location]\n",
    "        \n",
    "        if location == \"United States\":\n",
    "            return location\n",
    "            \n",
    "        if \", United States\" in location:\n",
    "            location = location.replace(\", United States\", \"\")\n",
    "        \n",
    "        location = re.sub(r' [Cc]ounty', '', location)\n",
    "        \n",
    "        if location in state_to_abbrev:\n",
    "            return state_to_abbrev[location]\n",
    "        \n",
    "        for state_name, abbrev in state_to_abbrev.items():\n",
    "            if f\", {state_name}\" in location:\n",
    "                return location.replace(f\", {state_name}\", f\", {abbrev}\")\n",
    "        \n",
    "        return location\n",
    "    \n",
    "    df['location'] = df['location'].apply(standardize_location)\n",
    "    \n",
    "    return df\n",
    "\n",
    "postings_with_salary = standardize_locations(postings_with_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "changed_locations = []\n",
    "for loc in postings_with_salary['location'].dropna().unique():\n",
    "   if not re.search(r', [A-Z]{2}$', str(loc)):\n",
    "       changed_locations.append(loc)\n",
    "\n",
    "# Sort and print them\n",
    "changed_locations.sort()\n",
    "print(len(changed_locations))\n",
    "\n",
    "print(f\"Locations without standard format ({len(changed_locations)} found):\")\n",
    "for loc in changed_locations:\n",
    "   print(f\"- {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#convert job_id to string for all dataframes\n",
    "postings_with_salary['job_id'] = postings_with_salary['job_id'].astype(str)\n",
    "jobs_industries['job_id'] = jobs_industries['job_id'].astype(str)\n",
    "job_skills['job_id'] = job_skills['job_id'].astype(str)\n",
    "\n",
    "grouped_industry_jobs = jobs_industries.groupby('job_id')['industry_id'].apply(\n",
    "    lambda x: ','.join(map(str, x))\n",
    ").reset_index()\n",
    "\n",
    "grouped_skills_jobs = job_skills.groupby('job_id')['skill_abr'].apply(\n",
    "    lambda x: ','.join(map(str, x))\n",
    ").reset_index()\n",
    "\n",
    "#merge postings with industries\n",
    "postings_with_salary = pd.merge(\n",
    "    postings_with_salary,\n",
    "    grouped_industry_jobs,\n",
    "    on='job_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "#merge with skills\n",
    "postings_with_salary = pd.merge(\n",
    "    postings_with_salary,\n",
    "    grouped_skills_jobs,\n",
    "    on='job_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "postings_with_salary['industry_id'] = postings_with_salary['industry_id'].fillna('')\n",
    "postings_with_salary['skill_abr'] = postings_with_salary['skill_abr'].fillna('')\n",
    "\n",
    "print(f\"Postings shape: {postings_with_salary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postings_with_salary['job_id'] = postings_with_salary['job_id'].astype(str)\n",
    "benefits['job_id'] = benefits['job_id'].astype(str)\n",
    "\n",
    "#looking at the benefits data, we can see that there are two types of benefits: inferred and non-inferred\n",
    "inferred_benefits_dict = {}\n",
    "non_inferred_benefits_dict = {}\n",
    "\n",
    "for job_id in postings_with_salary['job_id'].unique():\n",
    "    inferred_benefits_dict[job_id] = []\n",
    "    non_inferred_benefits_dict[job_id] = []\n",
    "\n",
    "job_benefits = benefits[benefits['job_id'].isin(postings_with_salary['job_id'])]\n",
    "\n",
    "for _, row in job_benefits.iterrows():\n",
    "    job_id = row['job_id']\n",
    "    benefit_type = row['type']\n",
    "    inferred = row['inferred']\n",
    "    \n",
    "    if inferred == 1:\n",
    "        inferred_benefits_dict[job_id].append(benefit_type)\n",
    "    else:\n",
    "        non_inferred_benefits_dict[job_id].append(benefit_type)\n",
    "\n",
    "def list_to_comma_str(lst):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \",\".join(lst)\n",
    "\n",
    "#create infered and non-inferred benefits columns\n",
    "postings_with_salary['inferred_benefits'] = postings_with_salary['job_id'].map(\n",
    "    lambda x: list_to_comma_str(inferred_benefits_dict.get(x, []))\n",
    ")\n",
    "postings_with_salary['non_inferred_benefits'] = postings_with_salary['job_id'].map(\n",
    "    lambda x: list_to_comma_str(non_inferred_benefits_dict.get(x, []))\n",
    ")\n",
    "\n",
    "print(f\"{postings_with_salary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_company_data.to_csv('processed_linkedin_companies.csv', index=False)\n",
    "postings_with_salary.to_csv('processed_linkedin_postings_salary.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
