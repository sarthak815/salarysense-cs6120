{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "eda_df = pd.read_csv('raw_glassdoor_data/eda_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_cols = [\n",
    "    'Company Name', 'Headquarters', 'Size', 'Founded', \n",
    "    'Type of ownership', 'Industry', 'Sector', 'Revenue', \n",
    "    'Competitors', 'company_txt', 'Rating'\n",
    "]\n",
    "\n",
    "companies_df = eda_df[company_cols].drop_duplicates(subset=['Company Name']).copy()\n",
    "companies_df.reset_index(drop=True, inplace=True)\n",
    "companies_df['company_id'] = companies_df.index + 1\n",
    "\n",
    "companies_df = companies_df[['company_id'] + company_cols]\n",
    "\n",
    "companies_df.drop(columns=['Company Name'], inplace=True)\n",
    "companies_df.rename(columns={'company_txt': 'Company Name'}, inplace=True)\n",
    "companies_df.rename(columns={'Rating': 'Company Rating'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Companies with more than one unique Industry:\n",
      "Empty DataFrame\n",
      "Columns: [clean_company_name, unique_industries]\n",
      "Index: []\n",
      "\n",
      "Companies with more than one unique Sector:\n",
      "Empty DataFrame\n",
      "Columns: [clean_company_name, unique_sectors]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a cleaned version of the company name by removing any appended rating.\n",
    "eda_df['clean_company_name'] = eda_df['Company Name'].apply(lambda x: x.split(\"\\n\")[0].strip())\n",
    "\n",
    "# Check for variation in the 'Industry' column.\n",
    "industry_variation = eda_df.groupby('clean_company_name')['Industry'].nunique().reset_index(name='unique_industries')\n",
    "companies_with_multiple_industries = industry_variation[industry_variation['unique_industries'] > 1]\n",
    "print(\"Companies with more than one unique Industry:\")\n",
    "print(companies_with_multiple_industries)\n",
    "\n",
    "# Check for variation in the 'Sector' column.\n",
    "sector_variation = eda_df.groupby('clean_company_name')['Sector'].nunique().reset_index(name='unique_sectors')\n",
    "companies_with_multiple_sectors = sector_variation[sector_variation['unique_sectors'] > 1]\n",
    "print(\"\\nCompanies with more than one unique Sector:\")\n",
    "print(companies_with_multiple_sectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_cols = [\n",
    "    'Job Title', 'Salary Estimate', 'Job Description', \n",
    "    'hourly', 'employer_provided',\n",
    "    'min_salary', 'max_salary', 'avg_salary', 'age',\n",
    "    'python_yn', 'R_yn', 'spark', 'aws', 'excel', \n",
    "    'job_simp', 'seniority', 'num_comp'\n",
    "]\n",
    "\n",
    "# Create the jobs DataFrame from eda_df, using the selected job columns.\n",
    "jobs_df = eda_df[job_cols].copy()\n",
    "\n",
    "# Add the clean Company Name from the 'company_txt' column, which you'll use for merging.\n",
    "jobs_df['Company Name'] = eda_df['company_txt']\n",
    "\n",
    "# Merge jobs_df with companies_df to get the corresponding company_id.\n",
    "# companies_df has been built using the clean company name from 'company_txt', renamed as 'Company Name'\n",
    "jobs_df = jobs_df.merge(companies_df[['company_id', 'Company Name']], on='Company Name', how='left')\n",
    "\n",
    "# Optionally, if you don't need the Company Name in jobs_df now that you have company_id, drop it:\n",
    "jobs_df.drop(columns=['Company Name'], inplace=True)\n",
    "\n",
    "# Create a unique job_id for each job (starting at 1)\n",
    "jobs_df['job_id'] = range(1, len(jobs_df) + 1)\n",
    "\n",
    "# Reorder the DataFrame to have job_id as the first column\n",
    "cols = ['job_id'] + [col for col in jobs_df.columns if col != 'job_id']\n",
    "jobs_df = jobs_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove the \"Salary Estimate\" column\n",
    "jobs_df.drop(columns=['Salary Estimate'], inplace=True)\n",
    "\n",
    "# Create masks for hourly and non-hourly jobs\n",
    "hourly_mask = jobs_df['hourly'] == 1\n",
    "non_hourly_mask = jobs_df['hourly'] == 0\n",
    "\n",
    "# For hourly jobs: convert hourly rate to annual salary by multiplying by 40*52 (2,080)\n",
    "jobs_df.loc[hourly_mask, 'min_salary'] = jobs_df.loc[hourly_mask, 'min_salary'] * (40 * 52)\n",
    "jobs_df.loc[hourly_mask, 'max_salary'] = jobs_df.loc[hourly_mask, 'max_salary'] * (40 * 52)\n",
    "jobs_df.loc[hourly_mask, 'avg_salary'] = jobs_df.loc[hourly_mask, 'avg_salary'] * (40 * 52)\n",
    "\n",
    "# For non-hourly jobs: convert from thousands to full dollars by multiplying by 1,000\n",
    "jobs_df.loc[non_hourly_mask, 'min_salary'] = jobs_df.loc[non_hourly_mask, 'min_salary'] * 1000\n",
    "jobs_df.loc[non_hourly_mask, 'max_salary'] = jobs_df.loc[non_hourly_mask, 'max_salary'] * 1000\n",
    "jobs_df.loc[non_hourly_mask, 'avg_salary'] = jobs_df.loc[non_hourly_mask, 'avg_salary'] * 1000\n",
    "\n",
    "# Drop the \"hourly\" column as it's no longer needed\n",
    "jobs_df.drop(columns=['hourly'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   job_id                  Job Title  \\\n",
      "0       1             Data Scientist   \n",
      "1       2  Healthcare Data Scientist   \n",
      "2       3             Data Scientist   \n",
      "3       4             Data Scientist   \n",
      "4       5             Data Scientist   \n",
      "\n",
      "                                     Job Description  employer_provided  \\\n",
      "0  Data Scientist\\nLocation: Albuquerque, NM\\nEdu...                  0   \n",
      "1  What You Will Do:\\n\\nI. General Summary\\n\\nThe...                  0   \n",
      "2  KnowBe4, Inc. is a high growth information sec...                  0   \n",
      "3  *Organization and Job ID**\\nJob ID: 310709\\n\\n...                  0   \n",
      "4  Data Scientist\\nAffinity Solutions / Marketing...                  0   \n",
      "\n",
      "   min_salary  max_salary  avg_salary  age  Python  R  Spark  AWS  Excel  \\\n",
      "0       53000       91000     72000.0   47       1  0      0    0      1   \n",
      "1       63000      112000     87500.0   36       1  0      0    0      0   \n",
      "2       80000       90000     85000.0   10       1  0      1    0      1   \n",
      "3       56000       97000     76500.0   55       1  0      0    0      0   \n",
      "4       86000      143000    114500.0   22       1  0      0    0      1   \n",
      "\n",
      "         job_simp seniority  num_comp  company_id  \n",
      "0  data scientist        na         0           1  \n",
      "1  data scientist        na         0           2  \n",
      "2  data scientist        na         0           3  \n",
      "3  data scientist        na         3           4  \n",
      "4  data scientist        na         3           5  \n"
     ]
    }
   ],
   "source": [
    "# Rename the columns in jobs_df\n",
    "jobs_df.rename(columns={\n",
    "    'python_yn': 'Python',\n",
    "    'R_yn': 'R',\n",
    "    'spark': 'Spark',\n",
    "    'aws': 'AWS',\n",
    "    'excel': 'Excel'\n",
    "}, inplace=True)\n",
    "\n",
    "# Verify the changes\n",
    "print(jobs_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation_factor = 1.314  # from your calculation\n",
    "\n",
    "# Apply the inflation factor\n",
    "jobs_df['min_salary'] = jobs_df['min_salary'] * inflation_factor\n",
    "jobs_df['max_salary'] = jobs_df['max_salary'] * inflation_factor\n",
    "jobs_df['avg_salary'] = jobs_df['avg_salary'] * inflation_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   job_id                  Job Title  \\\n",
      "0       1             Data Scientist   \n",
      "1       2  Healthcare Data Scientist   \n",
      "2       3             Data Scientist   \n",
      "3       4             Data Scientist   \n",
      "4       5             Data Scientist   \n",
      "\n",
      "                                     Job Description seniority  company_id  \\\n",
      "0  Data Scientist\\nLocation: Albuquerque, NM\\nEdu...        na           1   \n",
      "1  What You Will Do:\\n\\nI. General Summary\\n\\nThe...        na           2   \n",
      "2  KnowBe4, Inc. is a high growth information sec...        na           3   \n",
      "3  *Organization and Job ID**\\nJob ID: 310709\\n\\n...        na           4   \n",
      "4  Data Scientist\\nAffinity Solutions / Marketing...        na           5   \n",
      "\n",
      "   age  Python  R  Spark  AWS  Excel  min_salary  max_salary  avg_salary  \\\n",
      "0   47       1  0      0    0      1     69642.0    119574.0     94608.0   \n",
      "1   36       1  0      0    0      0     82782.0    147168.0    114975.0   \n",
      "2   10       1  0      1    0      1    105120.0    118260.0    111690.0   \n",
      "3   55       1  0      0    0      0     73584.0    127458.0    100521.0   \n",
      "4   22       1  0      0    0      1    113004.0    187902.0    150453.0   \n",
      "\n",
      "   employer_provided  num_comp  \n",
      "0                  0         0  \n",
      "1                  0         0  \n",
      "2                  0         0  \n",
      "3                  0         3  \n",
      "4                  0         3  \n"
     ]
    }
   ],
   "source": [
    "# Define the desired column order\n",
    "desired_order = [\n",
    "    'job_id', \n",
    "    'Job Title', \n",
    "    'Job Description', \n",
    "    'seniority', \n",
    "    'company_id', \n",
    "    'age', \n",
    "    'Python', \n",
    "    'R', \n",
    "    'Spark', \n",
    "    'AWS', \n",
    "    'Excel', \n",
    "    'min_salary', \n",
    "    'max_salary', \n",
    "    'avg_salary', \n",
    "    'employer_provided',\n",
    "    'num_comp'\n",
    "]\n",
    "\n",
    "# Reorder the DataFrame accordingly\n",
    "jobs_df = jobs_df[desired_order]\n",
    "\n",
    "# Verify the new order of columns\n",
    "print(jobs_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the jobs table to glassdoor_jobs.csv without the index\n",
    "jobs_df.to_csv(\"glassdoor_jobs.csv\", index=False)\n",
    "\n",
    "# Save the companies table to glassdoor_companies.csv without the index\n",
    "companies_df.to_csv(\"glassdoor_companies.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
