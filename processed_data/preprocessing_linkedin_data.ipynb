{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kagglehub\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = pd.read_csv('raw_linkedin_data/companies/companies.csv')\n",
    "employee_counts = pd.read_csv('raw_linkedin_data/companies/employee_counts.csv')\n",
    "industries = pd.read_csv('raw_linkedin_data/companies/company_industries.csv')\n",
    "specialities = pd.read_csv('raw_linkedin_data/companies/company_specialities.csv')\n",
    "\n",
    "industries_ids = pd.read_csv('raw_linkedin_data/mappings/industries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates present in companies.csv: False\n",
      "Duplicates present in employee_counts.csv: True\n",
      "Duplicates present in company_industries.csv: True\n",
      "Duplicates present in company_specialities.csv: True\n"
     ]
    }
   ],
   "source": [
    "#did a simple check to see if there are duplicates in the data regarding industries\n",
    "#this is important because if there are duplicates, we need to process them\n",
    "#in order to avoid any errors in the data\n",
    "\n",
    "companies_has_duplicates = companies['company_id'].duplicated().any()\n",
    "print(\"Duplicates present in companies.csv:\", companies_has_duplicates)\n",
    "\n",
    "employee_counts_has_duplicates = employee_counts['company_id'].duplicated().any()\n",
    "print(\"Duplicates present in employee_counts.csv:\", employee_counts_has_duplicates)\n",
    "\n",
    "industry_has_duplicates = industries['company_id'].duplicated().any()\n",
    "print(\"Duplicates present in company_industries.csv:\", industry_has_duplicates)\n",
    "\n",
    "specialities_has_duplicates = specialities['company_id'].duplicated().any()\n",
    "print(\"Duplicates present in company_specialities.csv:\", specialities_has_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_counts['time_recorded'] = pd.to_datetime(employee_counts['time_recorded'])\n",
    "employee_counts_sorted = employee_counts.sort_values(['company_id', 'time_recorded'], ascending=[True, False])\n",
    "employee_counts_unique = employee_counts_sorted.drop_duplicates(subset='company_id', keep='first')\n",
    "# Convert 'time_recorded' to datetime format\n",
    "# Sort the employee_counts dataframe by 'company_id' and 'record_date' in descending order\n",
    "# Drop duplicates, keeping the first occurrence (most recent record) for each 'company_id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All records are the most recent: True\n"
     ]
    }
   ],
   "source": [
    "#check to the earlier step to see if we are getting the most recent tie\n",
    "max_dates = employee_counts.groupby('company_id')['time_recorded'].max().reset_index().rename(columns={'time_recorded': 'max_date'})\n",
    "merged = employee_counts_unique.merge(max_dates, on='company_id')\n",
    "merged['is_most_recent'] = merged['time_recorded'] == merged['max_date']\n",
    "all_recent = merged['is_most_recent'].all()\n",
    "print(\"All records are the most recent:\", all_recent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       company_id  industry_id\n",
      "0          391906         82.0\n",
      "1        22292832         48.0\n",
      "2           20300         41.0\n",
      "3         3570660         82.0\n",
      "4          878353        104.0\n",
      "...           ...          ...\n",
      "24370       32313        143.0\n",
      "24371    15225088         96.0\n",
      "24372     2852377         31.0\n",
      "24373    19114724         48.0\n",
      "24374     8060959        116.0\n",
      "\n",
      "[24375 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "industries_with_ids = industries.merge(\n",
    "    industries_ids,\n",
    "    left_on='industry',\n",
    "    right_on='industry_name',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the original industry name columns (both from industries and the mapping)\n",
    "industries_with_ids = industries_with_ids.drop(columns=['industry', 'industry_name'])\n",
    "\n",
    "# Convert non-NaN values in 'industry_id' to int while leaving NaN values unchanged\n",
    "industries_with_ids['industry_id'] = industries_with_ids['industry_id'].apply(\n",
    "    lambda x: int(x) if pd.notnull(x) else x\n",
    ")\n",
    "\n",
    "print(industries_with_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialities['speciality'] = specialities['speciality'].astype(str).str.strip()\n",
    "\n",
    "aggregated_industries_list = industries_with_ids.groupby('company_id')['industry_id'].apply(\n",
    "    lambda x: sorted([i for i in list(x.unique()) if pd.notnull(i)])  # Filter out NaN values\n",
    ").reset_index()\n",
    "\n",
    "aggregated_specialities_list = specialities.groupby('company_id')['speciality'].apply(\n",
    "    lambda x: sorted(list(x.unique()))\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregation is correct for all companies in column 'industry_id'.\n",
      "Aggregation is correct for all companies in column 'speciality'.\n"
     ]
    }
   ],
   "source": [
    "def check_list_aggregation(df_original, df_aggregated, key_col, value_col, list_col):\n",
    "    errors = []\n",
    "    for idx, row in df_aggregated.iterrows():\n",
    "        company_id = row[key_col]\n",
    "        # Get unique values from the original dataframe for this company\n",
    "        original_values_with_nan = list(df_original[df_original[key_col] == company_id][value_col].unique())\n",
    "        # Filter out NaN values to match the aggregation logic\n",
    "        original_values = sorted([val for val in original_values_with_nan if pd.notnull(val)])\n",
    "        \n",
    "        # Get the aggregated list\n",
    "        aggregated_values = sorted(row[list_col])\n",
    "        \n",
    "        if aggregated_values != original_values:\n",
    "            errors.append((company_id, original_values, aggregated_values))\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"Errors found in aggregation ({len(errors)} companies):\")\n",
    "        for comp_id, orig, agg in errors[:5]:  # Print first 5 errors to avoid excessive output\n",
    "            print(f\"Company ID {comp_id}: Expected {orig}, Aggregated {agg}\")\n",
    "        if len(errors) > 5:\n",
    "            print(f\"...and {len(errors) - 5} more errors.\")\n",
    "    else:\n",
    "        print(f\"Aggregation is correct for all companies in column '{list_col}'.\")\n",
    "\n",
    "# Check aggregation for industries\n",
    "check_list_aggregation(\n",
    "    industries_with_ids,            # Original dataframe\n",
    "    aggregated_industries_list,    # Aggregated dataframe\n",
    "    'company_id',                  # Key column\n",
    "    'industry_id',                    # Value column in original data\n",
    "    'industry_id'                     # List column in aggregated data (before renaming)\n",
    ")\n",
    "\n",
    "# Check aggregation for specialities\n",
    "check_list_aggregation(\n",
    "    specialities,          # Original dataframe\n",
    "    aggregated_specialities_list,  # Aggregated dataframe\n",
    "    'company_id',                  # Key column\n",
    "    'speciality',                  # Value column in original data\n",
    "    'speciality'                   # List column in aggregated data (before renaming)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_company_data = companies.merge(employee_counts_unique, on='company_id', how='left') \\\n",
    "                        .merge(aggregated_industries_list, on='company_id', how='left') \\\n",
    "                        .merge(aggregated_specialities_list, on='company_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_id           0\n",
      "name                 1\n",
      "description        297\n",
      "company_size      2774\n",
      "state               22\n",
      "country              0\n",
      "city                 1\n",
      "zip_code            28\n",
      "address             22\n",
      "url                  0\n",
      "employee_count       0\n",
      "follower_count       0\n",
      "time_recorded        0\n",
      "industry_id        108\n",
      "speciality        6693\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(final_company_data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values are consistent with the original data: True\n"
     ]
    }
   ],
   "source": [
    "def check_missing_consistency(company_data, original_df, column_name):\n",
    "    \"\"\"\n",
    "    Returns True if all companies with NaN in company_data for the given column\n",
    "    are absent in the original_df (i.e., they have no record in the original source).\n",
    "    \n",
    "    Parameters:\n",
    "    - company_data: DataFrame that contains the aggregated column.\n",
    "    - original_df: DataFrame of the original data (industries or specialties).\n",
    "    - column_name: Name of the aggregated column in company_data ('industry' or 'speciality').\n",
    "    \n",
    "    Returns:\n",
    "    - Boolean: True if consistency holds, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the set of company_ids with missing aggregated values\n",
    "    missing_ids = set(company_data[company_data[column_name].isna()]['company_id'])\n",
    "    # Get the set of company_ids that appear in the original data\n",
    "    original_ids = set(original_df['company_id'].unique())\n",
    "    \n",
    "    # If a company has a missing value in the aggregated column but appears in the original data,\n",
    "    # that's a problem.\n",
    "    return len(missing_ids.intersection(original_ids)) == 0\n",
    "\n",
    "def check_missing_industries_and_specialties(company_data, industries_df, specialities_df):\n",
    "    \"\"\"\n",
    "    Checks missing data consistency for both industries and specialities columns.\n",
    "    \n",
    "    Returns True if both checks are True, otherwise False.\n",
    "    \"\"\"\n",
    "    industry_check = check_missing_consistency(company_data, industries_df, 'industry_id')\n",
    "    speciality_check = check_missing_consistency(company_data, specialities_df, 'speciality')\n",
    "    return industry_check and speciality_check\n",
    "\n",
    "# Example usage:\n",
    "result = check_missing_industries_and_specialties(final_company_data, industries_with_ids, specialities)\n",
    "print(\"Missing values are consistent with the original data:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the postings dataframe\n",
    "\n",
    "postings = pd.read_csv('raw_linkedin_data/postings.csv')\n",
    "jobs_industries = pd.read_csv('raw_linkedin_data/jobs/job_industries.csv')\n",
    "job_salaries = pd.read_csv('raw_linkedin_data/jobs/salaries.csv')\n",
    "job_skills = pd.read_csv('raw_linkedin_data/jobs/job_skills.csv')\n",
    "\n",
    "benefits = pd.read_csv('raw_linkedin_data/jobs/benefits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salary information in salaries.csv and posting.csv is the same: True\n"
     ]
    }
   ],
   "source": [
    "#has a salaries.csv file that contains the salary information for each job posting and postings.csv file that contains the job postings with salary information\n",
    "#want to make sure that the salary is \n",
    "\n",
    "def are_salaries_consistent(postings, salaries):\n",
    "    postings['job_id'] = postings['job_id'].astype(str)\n",
    "    salaries['job_id'] = salaries['job_id'].astype(str)\n",
    "    \n",
    "    postings_with_salary = postings[postings['max_salary'].notna() | postings['min_salary'].notna() | postings['med_salary'].notna()]\n",
    "    \n",
    "    posting_job_ids = set(postings_with_salary['job_id'])    \n",
    "    salary_job_ids = set(salaries['job_id'])\n",
    "    \n",
    "    if not posting_job_ids.issubset(salary_job_ids):\n",
    "        return False\n",
    "    \n",
    "    fields = ['max_salary', 'med_salary', 'min_salary', 'pay_period', 'currency', 'compensation_type']\n",
    "    \n",
    "    relevant_salaries = salaries[salaries['job_id'].isin(posting_job_ids)]\n",
    "    relevant_salaries = relevant_salaries.set_index('job_id')\n",
    "    \n",
    "    for _, post in postings_with_salary.iterrows():\n",
    "        job_id = post['job_id']\n",
    "        salary = relevant_salaries.loc[job_id]\n",
    "        \n",
    "        for field in fields:\n",
    "            if pd.isna(post[field]) and pd.isna(salary[field]):\n",
    "                continue\n",
    "                \n",
    "            if field in ['max_salary', 'med_salary', 'min_salary']:\n",
    "                if not pd.isna(post[field]) and not pd.isna(salary[field]):\n",
    "                    if abs(float(post[field]) - float(salary[field])) > 0.01:\n",
    "                        return False\n",
    "            elif str(post[field]).strip() != str(salary[field]).strip():\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"salary information in salaries.csv and posting.csv is the same:\", are_salaries_consistent(postings, job_salaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original postings: 123849\n",
      "Postings with salary information: 36073\n",
      "Percentage with salary info: 29.13%\n"
     ]
    }
   ],
   "source": [
    "def create_salary_dataset(postings):\n",
    "    salary_df = postings.copy()\n",
    "    \n",
    "    has_salary = (\n",
    "        salary_df['max_salary'].notna() | \n",
    "        salary_df['med_salary'].notna() | \n",
    "        salary_df['min_salary'].notna() |\n",
    "        salary_df['normalized_salary'].notna()\n",
    "    )\n",
    "    \n",
    "    has_period = salary_df['pay_period'].notna()\n",
    "    \n",
    "    salary_df = salary_df[has_salary & has_period]\n",
    "    \n",
    "    return salary_df\n",
    "\n",
    "postings_with_salary = create_salary_dataset(postings)\n",
    "print(f\"Original postings: {len(postings)}\")\n",
    "print(f\"Postings with salary information: {len(postings_with_salary)}\")\n",
    "print(f\"Percentage with salary info: {len(postings_with_salary)/len(postings)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All salaries are in USD: False\n",
      "currency\n",
      "USD    36058\n",
      "EUR        6\n",
      "CAD        3\n",
      "BBD        2\n",
      "AUD        2\n",
      "GBP        2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_usd = (postings_with_salary['currency'] == 'USD').all()\n",
    "print(f\"All salaries are in USD: {all_usd}\")\n",
    "\n",
    "# If you want to see the count of different currencies:\n",
    "currency_counts = postings_with_salary['currency'].value_counts()\n",
    "print(currency_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ID\tLocation\tCurrency\n",
      "__________________________________________________\n",
      "3885870216\tUnited States\tCAD\n",
      "3888974657\tReno, NV\tCAD\n",
      "3888974658\tKensett, AR\tBBD\n",
      "3888976416\tHuntsville, AL\tBBD\n",
      "3889710534\tGermany, PA\tEUR\n",
      "3889711148\tGermany, PA\tEUR\n",
      "3891804969\tNew York, United States\tAUD\n",
      "3894295381\tWhippany, NJ\tCAD\n",
      "3901991613\tNew York, United States\tAUD\n",
      "3902347641\tEnnis, TX\tEUR\n",
      "3903439951\tCenter City, MN\tEUR\n",
      "3903446331\tEngland, AR\tGBP\n",
      "3903449123\tEngland, AR\tGBP\n",
      "3903818935\tCenter City, MN\tEUR\n",
      "3903824359\tCenter City, MN\tEUR\n"
     ]
    }
   ],
   "source": [
    "non_usd_jobs = postings_with_salary[postings_with_salary['currency'] != 'USD']\n",
    "print(\"\\nID\\tLocation\\tCurrency\")\n",
    "print(\"_\"*50)\n",
    "for _, job in non_usd_jobs.iterrows():\n",
    "    print(f\"{job['job_id']}\\t{job['location']}\\t{job['currency']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All salaries converted to USD: True\n"
     ]
    }
   ],
   "source": [
    "#Since all the jobs that are not in USD are located in United States, we should convert the salaries to USD using these conversion rates.\n",
    "#1.00 US Dollar = 0.92367131 EUR\n",
    "#1.00 US Dollar = 1.4352374 CAD\n",
    "#1.00 US Dollar = 2.00 BBD\n",
    "#1.00 US Dollar = 1.594355 AUD\n",
    "#1.00 US Dollar = 0.77381294 GBP\n",
    "\n",
    "\n",
    "# Define conversion rates to USD (multiply by these to get USD)\n",
    "currency_to_usd = {\n",
    "    'EUR': 1 / 0.92367131,\n",
    "    'CAD': 1 / 1.4352374,\n",
    "    'BBD': 1 / 2.00,\n",
    "    'AUD': 1 / 1.594355,\n",
    "    'GBP': 1 / 0.77381294,\n",
    "    'USD': 1.0  # No conversion needed\n",
    "}\n",
    "\n",
    "# Function to convert salary to USD\n",
    "def convert_to_usd(row):\n",
    "    if row['currency'] != 'USD':\n",
    "        conversion_rate = currency_to_usd.get(row['currency'], 1.0)\n",
    "        \n",
    "        # Convert all salary fields including normalized_salary\n",
    "        for field in ['max_salary', 'med_salary', 'min_salary', 'normalized_salary']:\n",
    "            if field in row and pd.notna(row[field]):\n",
    "                row[field] = row[field] * conversion_rate\n",
    "        \n",
    "        # Update currency to USD\n",
    "        row['currency'] = 'USD'\n",
    "    \n",
    "    return row\n",
    "\n",
    "# Apply conversion to all rows\n",
    "postings_with_salary = postings_with_salary.apply(convert_to_usd, axis=1)\n",
    "\n",
    "# Verify all currencies are now USD\n",
    "all_usd = (postings_with_salary['currency'] == 'USD').all()\n",
    "print(f\"All salaries converted to USD: {all_usd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Full-time' 'Internship' 'Contract' 'Part-time' 'Temporary' 'Other'\n",
      " 'Volunteer']\n",
      "['HOURLY' 'YEARLY' 'MONTHLY' 'WEEKLY' 'BIWEEKLY']\n",
      "\n",
      "\n",
      "Count of hourly jobs by work type:\n",
      "formatted_work_type\n",
      "Full-time     8819\n",
      "Contract      3343\n",
      "Part-time     1955\n",
      "Temporary      332\n",
      "Internship     202\n",
      "Other           90\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_work_types = postings_with_salary['formatted_work_type'].unique()\n",
    "print(unique_work_types)\n",
    "\n",
    "unique_pay_periods = postings_with_salary['pay_period'].unique()\n",
    "print(unique_pay_periods)\n",
    "\n",
    "hourly_by_work_type = postings_with_salary[postings_with_salary['pay_period'] == 'HOURLY']['formatted_work_type'].value_counts()\n",
    "\n",
    "print(\"\\n\\nCount of hourly jobs by work type:\")\n",
    "print(hourly_by_work_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#after carefully examining the data, we can see that the normalized salary is sometimes taking the med_salary when available, sometimes average of max and min, and sometimes the max_salary when the med_salary is not available. \n",
    "#this column might not be of use to use since it is not consistent. \n",
    "\n",
    "# Drop the 'normalized_salary' column\n",
    "postings_with_salary = postings_with_salary.drop(columns=['normalized_salary'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_normalized_annual_salaries(df):\n",
    "    # Create new columns\n",
    "    df['norm_min_annual'] = df['min_salary'].copy()\n",
    "    df['norm_med_annual'] = df['med_salary'].copy()\n",
    "    df['norm_max_annual'] = df['max_salary'].copy()\n",
    "    \n",
    "    # Define multipliers for each pay period\n",
    "    pay_period_multipliers = {\n",
    "        'HOURLY': lambda row: 20 * 52 if row['work_type'] == 'PART_TIME' else 40 * 52,\n",
    "        'WEEKLY': 52,\n",
    "        'BIWEEKLY': 26,\n",
    "        'MONTHLY': 12,\n",
    "        'YEARLY': 1,\n",
    "        'ANNUAL': 1\n",
    "    }\n",
    "    \n",
    "    # Apply conversions\n",
    "    for idx, row in df.iterrows():\n",
    "        if row['pay_period'] in pay_period_multipliers:\n",
    "            multiplier = pay_period_multipliers[row['pay_period']]\n",
    "            if callable(multiplier):\n",
    "                multiplier = multiplier(row)\n",
    "                \n",
    "            for col in ['norm_min_annual', 'norm_med_annual', 'norm_max_annual']:\n",
    "                if pd.notna(row[col]):\n",
    "                    df.at[idx, col] = row[col] * multiplier\n",
    "    \n",
    "    # Fill missing med_salary with average of min and max\n",
    "    mask = pd.isna(df['norm_med_annual']) & pd.notna(df['norm_min_annual']) & pd.notna(df['norm_max_annual'])\n",
    "    df.loc[mask, 'norm_med_annual'] = (df.loc[mask, 'norm_min_annual'] + df.loc[mask, 'norm_max_annual']) / 2\n",
    "    \n",
    "    return df\n",
    "\n",
    "postings_with_salary = create_normalized_annual_salaries(postings_with_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "Locations without standard City, State format (240 found):\n",
      "- Alabama, United States\n",
      "- Alaska, United States\n",
      "- Albany, New York Metropolitan Area\n",
      "- Albuquerque, New Mexico, United States\n",
      "- Albuquerque-Santa Fe Metropolitan Area\n",
      "- Anchorage, Alaska, United States\n",
      "- Ann Arbor, Michigan, United States\n",
      "- Appleton-Oshkosh-Neenah Area\n",
      "- Arizona, United States\n",
      "- Arkansas, United States\n",
      "- Atlanta Metropolitan Area\n",
      "- Aurora, Colorado, United States\n",
      "- Austin, Texas Metropolitan Area\n",
      "- Austin, Texas, United States\n",
      "- Baton Rouge Metropolitan Area\n",
      "- Beaumont-Port Arthur Area\n",
      "- Bellingham Metropolitan Area\n",
      "- Bellingham, Washington, United States\n",
      "- Bend, Oregon, United States\n",
      "- Birkdale Village, North Carolina, United States\n",
      "- Blacksburg, Virginia, United States\n",
      "- Blacksburg-Christiansburg-Radford Area\n",
      "- Boise Metropolitan Area\n",
      "- Boston, Massachusetts, United States\n",
      "- Brooklyn, New York, United States\n",
      "- Broomfield, Colorado, United States\n",
      "- Buffalo-Niagara Falls Area\n",
      "- Calabasas, California, United States\n",
      "- California, United States\n",
      "- Cape Coral Metropolitan Area\n",
      "- Charleston, South Carolina Metropolitan Area\n",
      "- Charlotte Metro\n",
      "- Chicago, Illinois, United States\n",
      "- Cincinnati Metropolitan Area\n",
      "- Cleveland, Ohio, United States\n",
      "- College Station-Bryan Area\n",
      "- Colorado, United States\n",
      "- Columbia, South Carolina Metropolitan Area\n",
      "- Columbus, Georgia, United States\n",
      "- Columbus, Ohio Metropolitan Area\n",
      "- Columbus, Ohio, United States\n",
      "- Connecticut, United States\n",
      "- Crestview-Fort Walton Beach-Destin Area\n",
      "- Dallas, Texas, United States\n",
      "- Dallas-Fort Worth Metroplex\n",
      "- Delaware, United States\n",
      "- Denver Metropolitan Area\n",
      "- Denver, Colorado, United States\n",
      "- Des Moines Metropolitan Area\n",
      "- Detroit Metropolitan Area\n",
      "- District of Columbia, United States\n",
      "- Durham, North Carolina, United States\n",
      "- Eau Claire-Menomonie Area\n",
      "- Erie-Meadville Area\n",
      "- Fayetteville, North Carolina Metropolitan Area\n",
      "- Florida, United States\n",
      "- Foothill Ranch, California, United States\n",
      "- Fort Liberty, North Carolina, United States\n",
      "- Georgia, United States\n",
      "- Glendale, Arizona, United States\n",
      "- Grand Rapids Metropolitan Area\n",
      "- Greater Albany, Georgia Area\n",
      "- Greater Asheville\n",
      "- Greater Augusta Area\n",
      "- Greater Bend Area\n",
      "- Greater Birmingham, Alabama Area\n",
      "- Greater Bismarck Area\n",
      "- Greater Bloomington Area\n",
      "- Greater Boston\n",
      "- Greater Burlington Area\n",
      "- Greater Chattanooga\n",
      "- Greater Chicago Area\n",
      "- Greater Chico Area\n",
      "- Greater Cleveland\n",
      "- Greater Colorado Springs Area\n",
      "- Greater Corpus Christi Area\n",
      "- Greater Dothan\n",
      "- Greater Enid Area\n",
      "- Greater Eugene-Springfield Area\n",
      "- Greater Fayetteville, AR Area\n",
      "- Greater Flagstaff Area\n",
      "- Greater Fort Collins Area\n",
      "- Greater Fort Wayne\n",
      "- Greater Goldsboro Area\n",
      "- Greater Grand Junction Area\n",
      "- Greater Hartford\n",
      "- Greater Houston\n",
      "- Greater Indianapolis\n",
      "- Greater Jackson, MI Area\n",
      "- Greater Lansing\n",
      "- Greater Lexington Area\n",
      "- Greater Macon\n",
      "- Greater Madison Area\n",
      "- Greater McAllen Area\n",
      "- Greater Milwaukee\n",
      "- Greater Minneapolis-St. Paul Area\n",
      "- Greater Morgantown Area\n",
      "- Greater New Orleans Region\n",
      "- Greater Orlando\n",
      "- Greater Philadelphia\n",
      "- Greater Phoenix Area\n",
      "- Greater Pittsburgh Region\n",
      "- Greater Reno Area\n",
      "- Greater Richmond Region\n",
      "- Greater Sacramento\n",
      "- Greater San Luis Obispo Area\n",
      "- Greater Savannah Area\n",
      "- Greater Scranton Area\n",
      "- Greater Seattle Area\n",
      "- Greater Sioux Falls Area\n",
      "- Greater St. Louis\n",
      "- Greater Syracuse-Auburn Area\n",
      "- Greater Tampa Bay Area\n",
      "- Greater Tucson Area\n",
      "- Greater Wilmington Area\n",
      "- Green Bay, Wisconsin Metropolitan Area\n",
      "- Greensboro--Winston-Salem--High Point Area\n",
      "- Greenville-Spartanburg-Anderson, South Carolina Area\n",
      "- Hampton Roads, Virginia Metropolitan Area\n",
      "- Hawaii, United States\n",
      "- Hill Air Force Base, Utah, United States\n",
      "- Hilton Head Island, South Carolina Area\n",
      "- Honolulu Metropolitan Area\n",
      "- Honolulu, Hawaii, United States\n",
      "- Houston, Texas, United States\n",
      "- Idaho, United States\n",
      "- Illinois, United States\n",
      "- Indiana, United States\n",
      "- Iowa, United States\n",
      "- Johnson City-Kingsport-Bristol Area\n",
      "- Kansas City Metropolitan Area\n",
      "- Kansas, United States\n",
      "- Kentucky, United States\n",
      "- Knoxville Metropolitan Area\n",
      "- La Crosse-Onalaska Area\n",
      "- La Jolla, California, United States\n",
      "- Lafayette, Indiana Metropolitan Area\n",
      "- Lafayette, Louisiana Metropolitan Area\n",
      "- Laredo, Texas, United States\n",
      "- Las Vegas Metropolitan Area\n",
      "- Lawton Area\n",
      "- Lincoln, Nebraska Metropolitan Area\n",
      "- Little Rock Metropolitan Area\n",
      "- Long Beach, California, United States\n",
      "- Los Angeles Metropolitan Area\n",
      "- Los Angeles, California, United States\n",
      "- Louisiana, United States\n",
      "- Louisville Metropolitan Area\n",
      "- Lubbock-Levelland Area\n",
      "- Maine, United States\n",
      "- Maryland, United States\n",
      "- Massachusetts, United States\n",
      "- Maui\n",
      "- Memphis Metropolitan Area\n",
      "- Metro Jacksonville\n",
      "- Metropolitan Fresno\n",
      "- Miami-Fort Lauderdale Area\n",
      "- Michigan, United States\n",
      "- Milpitas-Northeastern, California, United States\n",
      "- Minnesota, United States\n",
      "- Mississippi, United States\n",
      "- Missouri, United States\n",
      "- Mobile Metropolitan Area\n",
      "- Modesto-Merced Area\n",
      "- Montana, United States\n",
      "- Morehead, North Carolina, United States\n",
      "- Nashville Metropolitan Area\n",
      "- Nebraska, United States\n",
      "- Nevada, United States\n",
      "- New Bern-Morehead City Area\n",
      "- New Hampshire, United States\n",
      "- New Jersey, United States\n",
      "- New Mexico, United States\n",
      "- New York City Metropolitan Area\n",
      "- New York, New York, United States\n",
      "- New York, United States\n",
      "- Newport Beach, California, United States\n",
      "- North Carolina, United States\n",
      "- North Dakota, United States\n",
      "- Northridge, California, United States\n",
      "- Ohio, United States\n",
      "- Oklahoma City Metropolitan Area\n",
      "- Oklahoma City, Oklahoma, United States\n",
      "- Oklahoma, United States\n",
      "- Omaha Metropolitan Area\n",
      "- Omaha, Nebraska, United States\n",
      "- Oregon, United States\n",
      "- Oxnard, California, United States\n",
      "- Pasco, Washington, United States\n",
      "- Pennsylvania, United States\n",
      "- Pensacola Metropolitan Area\n",
      "- Peoria Metropolitan Area\n",
      "- Portland, Maine Metropolitan Area\n",
      "- Portland, Oregon Metropolitan Area\n",
      "- Pueblo-Cañon City Area\n",
      "- Queens, New York, United States\n",
      "- Raleigh-Durham-Chapel Hill Area\n",
      "- Rhode Island, United States\n",
      "- Riesel, Texas, United States\n",
      "- Riverbank, California, United States\n",
      "- Rochester, New York Metropolitan Area\n",
      "- Rocky Mount-Wilson Area\n",
      "- Salt Lake City Metropolitan Area\n",
      "- San Antonio, Texas Metropolitan Area\n",
      "- San Diego Metropolitan Area\n",
      "- San Diego, California, United States\n",
      "- San Francisco Bay Area\n",
      "- San Francisco, California, United States\n",
      "- San Jose, California, United States\n",
      "- Santa Clarita, California, United States\n",
      "- Shamong, New Jersey, United States\n",
      "- South Bend-Mishawaka Region\n",
      "- South Carolina, United States\n",
      "- South Dakota, United States\n",
      "- Springfield, Illinois Metropolitan Area\n",
      "- Springfield, Massachusetts Metropolitan Area\n",
      "- St Petersburg, Florida, United States\n",
      "- Staten Island, New York, United States\n",
      "- Tallahassee Metropolitan Area\n",
      "- Tennessee, United States\n",
      "- Texas, United States\n",
      "- Thousand Oaks, California, United States\n",
      "- Toledo, Ohio Metropolitan Area\n",
      "- Topeka Metropolitan Area\n",
      "- Tulalip Bay, Washington, United States\n",
      "- Tulsa Metropolitan Area\n",
      "- United States\n",
      "- Utah, United States\n",
      "- Utica-Rome Area\n",
      "- Virginia, United States\n",
      "- Walla Walla Area\n",
      "- Washington DC-Baltimore Area\n",
      "- Washington, United States\n",
      "- Waterloo-Cedar Falls Area\n",
      "- West Virginia, United States\n",
      "- Whitestone, New York, United States\n",
      "- Wichita, Kansas Metropolitan Area\n",
      "- Wilmington, Delaware, United States\n",
      "- Wisconsin, United States\n",
      "- Youngstown-Warren area\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "non_standard_locations = []\n",
    "for loc in postings_with_salary['location'].dropna().unique():\n",
    "   if not re.search(r', [A-Z]{2}$', str(loc)):\n",
    "       non_standard_locations.append(loc)\n",
    "\n",
    "# Sort and print them\n",
    "non_standard_locations.sort()\n",
    "print(len(non_standard_locations))\n",
    "\n",
    "print(f\"Locations without standard City, State format ({len(non_standard_locations)} found):\")\n",
    "for loc in non_standard_locations:\n",
    "   print(f\"- {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_locations(df):\n",
    "    # Dictionary mapping state names to abbreviations \n",
    "    state_to_abbrev = {\n",
    "        'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', \n",
    "        'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',\n",
    "        'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID', \n",
    "        'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
    "        'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',\n",
    "        'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n",
    "        'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',\n",
    "        'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',\n",
    "        'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
    "        'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "        'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n",
    "        'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',\n",
    "        'Wisconsin': 'WI', 'Wyoming': 'WY', 'District of Columbia': 'DC'\n",
    "    }\n",
    "    \n",
    "    # Dictionary mapping metro areas to city, state format\n",
    "    metro_to_city_state = {\n",
    "    # A\n",
    "    \"Albany, New York Metropolitan Area\": \"Albany, NY\",\n",
    "    \"Albuquerque-Santa Fe Metropolitan Area\": \"Albuquerque, NM\",\n",
    "    \"Appleton-Oshkosh-Neenah Area\": \"Appleton, WI\",\n",
    "    \"Atlanta Metropolitan Area\": \"Atlanta, GA\",\n",
    "    \"Austin, Texas Metropolitan Area\": \"Austin, TX\",\n",
    "    \n",
    "    # B\n",
    "    \"Baton Rouge Metropolitan Area\": \"Baton Rouge, LA\",\n",
    "    \"Beaumont-Port Arthur Area\": \"Beaumont, TX\",\n",
    "    \"Bellingham Metropolitan Area\": \"Bellingham, WA\",\n",
    "    \"Blacksburg-Christiansburg-Radford Area\": \"Blacksburg, VA\",\n",
    "    \"Boise Metropolitan Area\": \"Boise, ID\",\n",
    "    \"Buffalo-Niagara Falls Area\": \"Buffalo, NY\",\n",
    "    \n",
    "    # C\n",
    "    \"Cape Coral Metropolitan Area\": \"Cape Coral, FL\",\n",
    "    \"Charleston, South Carolina Metropolitan Area\": \"Charleston, SC\",\n",
    "    \"Charlotte Metro\": \"Charlotte, NC\",\n",
    "    \"Cincinnati Metropolitan Area\": \"Cincinnati, OH\",\n",
    "    \"College Station-Bryan Area\": \"College Station, TX\",\n",
    "    \"Columbia, South Carolina Metropolitan Area\": \"Columbia, SC\",\n",
    "    \"Columbus, Ohio Metropolitan Area\": \"Columbus, OH\",\n",
    "    \"Crestview-Fort Walton Beach-Destin Area\": \"Fort Walton Beach, FL\",\n",
    "    \n",
    "    # D\n",
    "    \"Dallas-Fort Worth Metroplex\": \"Dallas, TX\",\n",
    "    \"Denver Metropolitan Area\": \"Denver, CO\",\n",
    "    \"Des Moines Metropolitan Area\": \"Des Moines, IA\",\n",
    "    \"Detroit Metropolitan Area\": \"Detroit, MI\",\n",
    "    \n",
    "    # E\n",
    "    \"Eau Claire-Menomonie Area\": \"Eau Claire, WI\",\n",
    "    \"Erie-Meadville Area\": \"Erie, PA\",\n",
    "    \n",
    "    # F\n",
    "    \"Fayetteville, North Carolina Metropolitan Area\": \"Fayetteville, NC\",\n",
    "    \n",
    "    # G\n",
    "    \"Grand Rapids Metropolitan Area\": \"Grand Rapids, MI\",\n",
    "    \"Greater Albany, Georgia Area\": \"Albany, GA\",\n",
    "    \"Greater Asheville\": \"Asheville, NC\",\n",
    "    \"Greater Augusta Area\": \"Augusta, GA\",\n",
    "    \"Greater Bend Area\": \"Bend, OR\",\n",
    "    \"Greater Birmingham, Alabama Area\": \"Birmingham, AL\",\n",
    "    \"Greater Bismarck Area\": \"Bismarck, ND\",\n",
    "    \"Greater Bloomington Area\": \"Bloomington, IN\",\n",
    "    \"Greater Boston\": \"Boston, MA\",\n",
    "    \"Greater Burlington Area\": \"Burlington, VT\",\n",
    "    \"Greater Chattanooga\": \"Chattanooga, TN\",\n",
    "    \"Greater Chicago Area\": \"Chicago, IL\",\n",
    "    \"Greater Chico Area\": \"Chico, CA\",\n",
    "    \"Greater Cleveland\": \"Cleveland, OH\",\n",
    "    \"Greater Colorado Springs Area\": \"Colorado Springs, CO\",\n",
    "    \"Greater Corpus Christi Area\": \"Corpus Christi, TX\",\n",
    "    \"Greater Dothan\": \"Dothan, AL\",\n",
    "    \"Greater Enid Area\": \"Enid, OK\",\n",
    "    \"Greater Eugene-Springfield Area\": \"Eugene, OR\",\n",
    "    \"Greater Fayetteville, AR Area\": \"Fayetteville, AR\",\n",
    "    \"Greater Flagstaff Area\": \"Flagstaff, AZ\",\n",
    "    \"Greater Fort Collins Area\": \"Fort Collins, CO\",\n",
    "    \"Greater Fort Wayne\": \"Fort Wayne, IN\",\n",
    "    \"Greater Goldsboro Area\": \"Goldsboro, NC\",\n",
    "    \"Greater Grand Junction Area\": \"Grand Junction, CO\",\n",
    "    \"Greater Hartford\": \"Hartford, CT\",\n",
    "    \"Greater Houston\": \"Houston, TX\",\n",
    "    \"Greater Indianapolis\": \"Indianapolis, IN\",\n",
    "    \"Greater Jackson, MI Area\": \"Jackson, MI\",\n",
    "    \"Greater Lansing\": \"Lansing, MI\",\n",
    "    \"Greater Lexington Area\": \"Lexington, KY\",\n",
    "    \"Greater Macon\": \"Macon, GA\",\n",
    "    \"Greater Madison Area\": \"Madison, WI\",\n",
    "    \"Greater McAllen Area\": \"McAllen, TX\",\n",
    "    \"Greater Milwaukee\": \"Milwaukee, WI\",\n",
    "    \"Greater Minneapolis-St. Paul Area\": \"Minneapolis, MN\",\n",
    "    \"Greater Morgantown Area\": \"Morgantown, WV\",\n",
    "    \"Greater New Orleans Region\": \"New Orleans, LA\",\n",
    "    \"Greater Orlando\": \"Orlando, FL\",\n",
    "    \"Greater Philadelphia\": \"Philadelphia, PA\",\n",
    "    \"Greater Phoenix Area\": \"Phoenix, AZ\",\n",
    "    \"Greater Pittsburgh Region\": \"Pittsburgh, PA\",\n",
    "    \"Greater Reno Area\": \"Reno, NV\",\n",
    "    \"Greater Richmond Region\": \"Richmond, VA\",\n",
    "    \"Greater Sacramento\": \"Sacramento, CA\",\n",
    "    \"Greater San Luis Obispo Area\": \"San Luis Obispo, CA\",\n",
    "    \"Greater Savannah Area\": \"Savannah, GA\",\n",
    "    \"Greater Scranton Area\": \"Scranton, PA\",\n",
    "    \"Greater Seattle Area\": \"Seattle, WA\",\n",
    "    \"Greater Sioux Falls Area\": \"Sioux Falls, SD\",\n",
    "    \"Greater St. Louis\": \"St. Louis, MO\",\n",
    "    \"Greater Syracuse-Auburn Area\": \"Syracuse, NY\",\n",
    "    \"Greater Tampa Bay Area\": \"Tampa, FL\",\n",
    "    \"Greater Tucson Area\": \"Tucson, AZ\",\n",
    "    \"Greater Wilmington Area\": \"Wilmington, DE\",\n",
    "    \"Green Bay, Wisconsin Metropolitan Area\": \"Green Bay, WI\",\n",
    "    \"Greensboro--Winston-Salem--High Point Area\": \"Greensboro, NC\",\n",
    "    \"Greenville-Spartanburg-Anderson, South Carolina Area\": \"Greenville, SC\",\n",
    "    \n",
    "    # H\n",
    "    \"Hampton Roads, Virginia Metropolitan Area\": \"Norfolk, VA\",\n",
    "    \"Hilton Head Island, South Carolina Area\": \"Hilton Head Island, SC\",\n",
    "    \"Honolulu Metropolitan Area\": \"Honolulu, HI\",\n",
    "    \n",
    "    # J\n",
    "    \"Johnson City-Kingsport-Bristol Area\": \"Johnson City, TN\",\n",
    "    \n",
    "    # K\n",
    "    \"Kansas City Metropolitan Area\": \"Kansas City, MO\",\n",
    "    \"Knoxville Metropolitan Area\": \"Knoxville, TN\",\n",
    "    \n",
    "    # L\n",
    "    \"La Crosse-Onalaska Area\": \"La Crosse, WI\",\n",
    "    \"Lafayette, Indiana Metropolitan Area\": \"Lafayette, IN\",\n",
    "    \"Lafayette, Louisiana Metropolitan Area\": \"Lafayette, LA\",\n",
    "    \"Las Vegas Metropolitan Area\": \"Las Vegas, NV\",\n",
    "    \"Lawton Area\": \"Lawton, OK\",\n",
    "    \"Lincoln, Nebraska Metropolitan Area\": \"Lincoln, NE\",\n",
    "    \"Little Rock Metropolitan Area\": \"Little Rock, AR\",\n",
    "    \"Los Angeles Metropolitan Area\": \"Los Angeles, CA\",\n",
    "    \"Louisville Metropolitan Area\": \"Louisville, KY\",\n",
    "    \"Lubbock-Levelland Area\": \"Lubbock, TX\",\n",
    "    \n",
    "    # M\n",
    "    \"Maui\": \"Lahaina, HI\",\n",
    "    \"Memphis Metropolitan Area\": \"Memphis, TN\",\n",
    "    \"Metro Jacksonville\": \"Jacksonville, FL\",\n",
    "    \"Metropolitan Fresno\": \"Fresno, CA\",\n",
    "    \"Miami-Fort Lauderdale Area\": \"Miami, FL\",\n",
    "    \"Mobile Metropolitan Area\": \"Mobile, AL\",\n",
    "    \"Modesto-Merced Area\": \"Modesto, CA\",\n",
    "    \n",
    "    # N\n",
    "    \"Nashville Metropolitan Area\": \"Nashville, TN\",\n",
    "    \"New Bern-Morehead City Area\": \"New Bern, NC\",\n",
    "    \"New York City Metropolitan Area\": \"New York, NY\",\n",
    "    \n",
    "    # O\n",
    "    \"Oklahoma City Metropolitan Area\": \"Oklahoma City, OK\",\n",
    "    \"Omaha Metropolitan Area\": \"Omaha, NE\",\n",
    "    \n",
    "    # P\n",
    "    \"Pensacola Metropolitan Area\": \"Pensacola, FL\",\n",
    "    \"Peoria Metropolitan Area\": \"Peoria, IL\",\n",
    "    \"Portland, Maine Metropolitan Area\": \"Portland, ME\",\n",
    "    \"Portland, Oregon Metropolitan Area\": \"Portland, OR\",\n",
    "    \"Pueblo-Cañon City Area\": \"Pueblo, CO\",\n",
    "    \n",
    "    # R\n",
    "    \"Raleigh-Durham-Chapel Hill Area\": \"Raleigh, NC\",\n",
    "    \"Rochester, New York Metropolitan Area\": \"Rochester, NY\",\n",
    "    \"Rocky Mount-Wilson Area\": \"Rocky Mount, NC\",\n",
    "    \n",
    "    # S\n",
    "    \"Salt Lake City Metropolitan Area\": \"Salt Lake City, UT\",\n",
    "    \"San Antonio, Texas Metropolitan Area\": \"San Antonio, TX\",\n",
    "    \"San Diego Metropolitan Area\": \"San Diego, CA\",\n",
    "    \"San Francisco Bay Area\": \"San Francisco, CA\",\n",
    "    \"South Bend-Mishawaka Region\": \"South Bend, IN\",\n",
    "    \"Springfield, Illinois Metropolitan Area\": \"Springfield, IL\",\n",
    "    \"Springfield, Massachusetts Metropolitan Area\": \"Springfield, MA\",\n",
    "    \n",
    "    # T\n",
    "    \"Tallahassee Metropolitan Area\": \"Tallahassee, FL\",\n",
    "    \"Toledo, Ohio Metropolitan Area\": \"Toledo, OH\",\n",
    "    \"Topeka Metropolitan Area\": \"Topeka, KS\",\n",
    "    \"Tulsa Metropolitan Area\": \"Tulsa, OK\",\n",
    "    \n",
    "    # U\n",
    "    \"Utica-Rome Area\": \"Utica, NY\",\n",
    "    \n",
    "    # W\n",
    "    \"Washington DC-Baltimore Area\": \"Washington, DC\",\n",
    "    \"Waterloo-Cedar Falls Area\": \"Waterloo, IA\",\n",
    "    \"Wichita, Kansas Metropolitan Area\": \"Wichita, KS\",\n",
    "    \"Walla Walla Area\" : \"Walla Walla, WA\",\n",
    "    \n",
    "    # Y\n",
    "    \"Youngstown-Warren area\": \"Youngstown, OH\"\n",
    "}\n",
    "    \n",
    "    def standardize_location(location):\n",
    "        if pd.isna(location):\n",
    "            return location\n",
    "            \n",
    "        location = str(location).strip()\n",
    "        \n",
    "        # First check if the location is in our metro area mapping\n",
    "        if location in metro_to_city_state:\n",
    "            return metro_to_city_state[location]\n",
    "        \n",
    "        # If location is just \"United States\", keep it as is\n",
    "        if location == \"United States\":\n",
    "            return location\n",
    "            \n",
    "        # Remove \", United States\" if it exists with other information\n",
    "        if \", United States\" in location:\n",
    "            location = location.replace(\", United States\", \"\")\n",
    "        \n",
    "        # Remove \" County\" or \" county\"\n",
    "        location = re.sub(r' [Cc]ounty', '', location)\n",
    "        \n",
    "        # Check if it's a state name by itself\n",
    "        if location in state_to_abbrev:\n",
    "            return state_to_abbrev[location]\n",
    "        \n",
    "        # Check for city, state name patterns\n",
    "        for state_name, abbrev in state_to_abbrev.items():\n",
    "            if f\", {state_name}\" in location:\n",
    "                # Replace state name with abbreviation\n",
    "                return location.replace(f\", {state_name}\", f\", {abbrev}\")\n",
    "        \n",
    "        # Return the original location if no transformations apply\n",
    "        return location\n",
    "    \n",
    "    # Apply standardization directly to the location column\n",
    "    df['location'] = df['location'].apply(standardize_location)\n",
    "    \n",
    "    return df\n",
    "\n",
    "postings_with_salary = standardize_locations(postings_with_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "Locations without standard City, State format (50 found):\n",
      "- AK\n",
      "- AL\n",
      "- AR\n",
      "- AZ\n",
      "- CA\n",
      "- CO\n",
      "- CT\n",
      "- DC\n",
      "- DE\n",
      "- FL\n",
      "- GA\n",
      "- HI\n",
      "- IA\n",
      "- ID\n",
      "- IL\n",
      "- IN\n",
      "- KS\n",
      "- KY\n",
      "- LA\n",
      "- MA\n",
      "- MD\n",
      "- ME\n",
      "- MI\n",
      "- MN\n",
      "- MO\n",
      "- MS\n",
      "- MT\n",
      "- NC\n",
      "- ND\n",
      "- NE\n",
      "- NH\n",
      "- NJ\n",
      "- NM\n",
      "- NV\n",
      "- NY\n",
      "- OH\n",
      "- OK\n",
      "- OR\n",
      "- PA\n",
      "- RI\n",
      "- SC\n",
      "- SD\n",
      "- TN\n",
      "- TX\n",
      "- UT\n",
      "- United States\n",
      "- VA\n",
      "- WA\n",
      "- WI\n",
      "- WV\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "non_standard_locations = []\n",
    "for loc in postings_with_salary['location'].dropna().unique():\n",
    "   if not re.search(r', [A-Z]{2}$', str(loc)):\n",
    "       non_standard_locations.append(loc)\n",
    "\n",
    "# Sort and print them\n",
    "non_standard_locations.sort()\n",
    "print(len(non_standard_locations))\n",
    "\n",
    "print(f\"Locations without standard City, State format ({len(non_standard_locations)} found):\")\n",
    "for loc in non_standard_locations:\n",
    "   print(f\"- {loc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added industry_id and skill_abr columns to postings\n",
      "Postings shape: (36073, 35)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you've already loaded these:\n",
    "# postings = pd.read_csv('raw_linkedin_data/postings.csv')\n",
    "# jobs_industries = pd.read_csv('raw_linkedin_data/jobs/job_industries.csv')\n",
    "# job_skills = pd.read_csv('raw_linkedin_data/jobs/job_skills.csv')\n",
    "\n",
    "# Convert job_id to string for consistent joining\n",
    "postings_with_salary['job_id'] = postings_with_salary['job_id'].astype(str)\n",
    "jobs_industries['job_id'] = jobs_industries['job_id'].astype(str)\n",
    "job_skills['job_id'] = job_skills['job_id'].astype(str)\n",
    "\n",
    "# Group industries by job_id and create a comma-separated list\n",
    "jobs_industries_grouped = jobs_industries.groupby('job_id')['industry_id'].apply(\n",
    "    lambda x: ','.join(map(str, x))\n",
    ").reset_index()\n",
    "\n",
    "# Group skills by job_id and create a comma-separated list\n",
    "job_skills_grouped = job_skills.groupby('job_id')['skill_abr'].apply(\n",
    "    lambda x: ','.join(map(str, x))\n",
    ").reset_index()\n",
    "\n",
    "# Merge postings with industries\n",
    "postings_with_salary = pd.merge(\n",
    "    postings_with_salary,\n",
    "    jobs_industries_grouped,\n",
    "    on='job_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge with skills\n",
    "postings_with_salary = pd.merge(\n",
    "    postings_with_salary,\n",
    "    job_skills_grouped,\n",
    "    on='job_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Replace NaN values with empty strings for the new columns\n",
    "postings_with_salary['industry_id'] = postings_with_salary['industry_id'].fillna('')\n",
    "postings_with_salary['skill_abr'] = postings_with_salary['skill_abr'].fillna('')\n",
    "\n",
    "# Now postings has the two new columns: industry_id and skill_abr\n",
    "print(f\"Added industry_id and skill_abr columns to postings\")\n",
    "print(f\"Postings shape: {postings_with_salary.shape}\")\n",
    "\n",
    "# If you want to save the enriched data\n",
    "# postings.to_csv('enriched_postings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added separated benefit columns to postings_with_salary\n",
      "Final shape: (36073, 37)\n"
     ]
    }
   ],
   "source": [
    "postings_with_salary['job_id'] = postings_with_salary['job_id'].astype(str)\n",
    "benefits['job_id'] = benefits['job_id'].astype(str)\n",
    "\n",
    "# Create dictionaries to store the separated benefits\n",
    "inferred_benefits_dict = {}\n",
    "non_inferred_benefits_dict = {}\n",
    "\n",
    "# Initialize empty lists for all job_ids\n",
    "for job_id in postings_with_salary['job_id'].unique():\n",
    "    inferred_benefits_dict[job_id] = []\n",
    "    non_inferred_benefits_dict[job_id] = []\n",
    "\n",
    "# Get benefits that match job_ids in postings_with_salary\n",
    "job_benefits = benefits[benefits['job_id'].isin(postings_with_salary['job_id'])]\n",
    "\n",
    "# Fill the dictionaries\n",
    "for _, row in job_benefits.iterrows():\n",
    "    job_id = row['job_id']\n",
    "    benefit_type = row['type']\n",
    "    inferred = row['inferred']\n",
    "    \n",
    "    if inferred == 1:\n",
    "        inferred_benefits_dict[job_id].append(benefit_type)\n",
    "    else:  # inferred == 0\n",
    "        non_inferred_benefits_dict[job_id].append(benefit_type)\n",
    "\n",
    "# Function to convert list to comma-separated string\n",
    "def list_to_comma_str(lst):\n",
    "    if not lst:\n",
    "        return \"\"\n",
    "    return \",\".join(lst)\n",
    "\n",
    "# Create the new columns using the dictionaries\n",
    "postings_with_salary['inferred_benefits'] = postings_with_salary['job_id'].map(\n",
    "    lambda x: list_to_comma_str(inferred_benefits_dict.get(x, []))\n",
    ")\n",
    "postings_with_salary['non_inferred_benefits'] = postings_with_salary['job_id'].map(\n",
    "    lambda x: list_to_comma_str(non_inferred_benefits_dict.get(x, []))\n",
    ")\n",
    "\n",
    "print(f\"Added separated benefit columns to postings_with_salary\")\n",
    "print(f\"Final shape: {postings_with_salary.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final_company_data to a CSV file\n",
    "final_company_data.to_csv('processed_linkedin_companies.csv', index=False)\n",
    "\n",
    "# Save postings_with_salary to a CSV file\n",
    "postings_with_salary.to_csv('processed_linkedin_postings_salary.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
